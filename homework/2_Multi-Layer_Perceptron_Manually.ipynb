{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-class exercise: implement \"accuracy\" - number of images correctly labeled\n",
    "\n",
    "## In-class exercise: split the training data into training & validation, and track validation loss during the training loop. \n",
    "\n",
    "Tip: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "- How do we know when to stop training? For example, you might stop when the validation loss stops improving.\n",
    "- Now that we have the basics, we can experiment with more complicated networks. Rather than implement these all by hand, we will move to using existing Python packages next week.\n",
    "- There are variants of \"universal approximation theorems\" roughly stating that there exists a nonlinear neural network with one hidden layer (possibly very wide) can fit an \"arbitrary\" nice/smooth function arbitrarily well. However, we can make the optimization easier with fancier layers than \"fully connected,\" like convolutional layers, which we will learn about next week. \n",
    "\n",
    "## Homework: improve the accuracy of this model. \n",
    "\n",
    "Update this notebook so that the accuracy is improved. How high can you get it? You could change things directly in the notebook, such as increasing the number of epochs, changing the learning weight, changing the width of the hidden layer, etc. If you're more ambitious, you could also try changing the model definition itself by checking out the associated Python files. For example, you could add more layers to the network. The current notebook has a training accuracy of about 43%, but will vary with randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/AlexLeeSmith/ai-science-training-series.git\n",
    "# %cd ai-science-training-series/02_neural_networks_python/\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from numpy import power, argmax, float32, prod, int32, arange, zeros, random\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from fc_net import TwoLayerNet\n",
    "\n",
    "def calc_decay(t, y0, decayRate: float):\n",
    "    \"\"\"\n",
    "    Calculates the value at a specified time point of an exponential decay function.\n",
    "\n",
    "    Side Effects: None\n",
    "    \"\"\"\n",
    "    return y0 * power(1 - decayRate, t)\n",
    "\n",
    "def sgd(model, gradients, learningRate: float):\n",
    "    \"\"\"\n",
    "    Performs one iteration of stochastic gradient decent (SGD) for each \n",
    "    parameter in the model.\n",
    "\n",
    "    Side Effects:\n",
    "    - The models weights will be changed.\n",
    "    \"\"\"\n",
    "    # Iterate over each parameter in the model.\n",
    "    for key, weights in model.params.items():\n",
    "        # Get the adjusted weights (gradients) for the current key.\n",
    "        dw = gradients[key]\n",
    "\n",
    "        # Apply these gradients to the current weights.\n",
    "        model.params[key] = weights - learningRate * dw\n",
    "\n",
    "    return model\n",
    "\n",
    "def calc_accuracy(model, x, trueValues):\n",
    "    \"\"\"\n",
    "    Calculates the percentage of matching values between x and trueValues.\n",
    "\n",
    "    Side Effects: None\n",
    "    \"\"\"\n",
    "    # Get the scores of each x input to each output class.\n",
    "    scores = model.loss(x) # [x.shape[0], model.num_classes]\n",
    "\n",
    "    # Select the output class with the highest score for each x input\n",
    "    predictions = argmax(scores, axis=1)\n",
    "\n",
    "    # Calculate the percentage of correct predictions.\n",
    "    accuracy = (trueValues == predictions).sum() / predictions.shape[0]\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# ===================================================================== #\n",
    "\n",
    "\n",
    "learningRate = 0.7      # Initial rate at which the model learns from each batch.\n",
    "decayRate = 0.005       # Rate at which the learning rate decays.\n",
    "numHiddenDim = 300      # Number of nodes in the hidden layer.\n",
    "modelWeightScale = 0.1  # Standard deviation for random initialization of the model weights.\n",
    "numEpochs = 5           # Number of times the entire dataset will be iterated over.\n",
    "# delta = 2e-3\n",
    "batchSizeTrain = 10000  # Size of each training data batch.\n",
    "\n",
    "# Load and split the MNIST data set.\n",
    "(xTrain, yTrain), (xTest, yTest) = mnist.load_data()\n",
    "\n",
    "# Do some pre-processing on the images.\n",
    "## Convert pixel values from integer to float32.\n",
    "xTrain = xTrain.astype(float32)\n",
    "xTest  = xTest.astype(float32)\n",
    "\n",
    "## Normalize the pixels to be within 0 and 1.\n",
    "xTrain /= 255.\n",
    "xTest  /= 255.\n",
    "\n",
    "## Flatten each image to a vector.\n",
    "xTrain = xTrain.reshape(xTrain.shape[0], prod(xTrain[0,:,:].shape)) # [60000, 28 * 28]\n",
    "xTest = xTest.reshape(xTest.shape[0], prod(xTest[0,:,:].shape))\n",
    "\n",
    "## Set the correct classifications as integers.\n",
    "yTrain = yTrain.astype(int32) # [60000,] => (0, 1, ..., or 9)\n",
    "yTest  = yTest.astype(int32)\n",
    "\n",
    "# Split the training data into training and validation.\n",
    "xTrain, xValid, yTrain, yValid = train_test_split(xTrain, yTrain, test_size=0.1)\n",
    "\n",
    "# One-hot encoding.\n",
    "nbClasses = 10\n",
    "yTrainOnehot = to_categorical(yTrain, nbClasses)\n",
    "yValidOnehot = to_categorical(yValid, nbClasses)\n",
    "yTestOnehot = to_categorical(yTest, nbClasses)\n",
    "\n",
    "# Calculate batches.\n",
    "batchesPerEpoch = int(xTrain.shape[0] / batchSizeTrain)\n",
    "batchSizeValid = int(xValid.shape[0] / batchesPerEpoch)\n",
    "\n",
    "# Generate necessary arrays.\n",
    "indicesTrain = arange(xTrain.shape[0])\n",
    "lossesTrain = zeros(batchesPerEpoch * numEpochs,)\n",
    "accTrain = zeros(batchesPerEpoch * numEpochs,)\n",
    "\n",
    "indicesValid = arange(xValid.shape[0])\n",
    "lossesValid = zeros(batchesPerEpoch * numEpochs,)\n",
    "accValid = zeros(batchesPerEpoch * numEpochs,)\n",
    "\n",
    "# Generate the model as a two-layer network.\n",
    "model = TwoLayerNet(input_dim=xTrain.shape[1], hidden_dim=numHiddenDim, num_classes=nbClasses, weight_scale=modelWeightScale)\n",
    "\n",
    "# Iterate over the training data to update the model's weights.\n",
    "batchCount = 0\n",
    "for curEpoch in range(0, numEpochs):\n",
    "    for curBatch in range(0, batchesPerEpoch):\n",
    "        # Get the next batch of data (training and validation).\n",
    "        offsetTrain = curBatch * batchSizeTrain\n",
    "        batchRangeTrain = range(offsetTrain, offsetTrain + batchSizeTrain)\n",
    "        xTrainBatch = xTrain[batchRangeTrain, :]\n",
    "        yTrainBatch = yTrain[batchRangeTrain]\n",
    "        yTrainOnehotBatch = yTrainOnehot[batchRangeTrain,:]\n",
    "\n",
    "        offsetValid = curBatch * batchSizeValid\n",
    "        batchRangeValid = range(offsetValid, offsetValid + batchSizeValid)        \n",
    "        xValidBatch = xValid[batchRangeValid, :]\n",
    "        yValidBatch = yValid[batchRangeValid]\n",
    "        yValidOnehotBatch = yValidOnehot[batchRangeValid,:]\n",
    "        \n",
    "        # Calculate the losses.\n",
    "        lossTrain, gradientsTrain = model.loss(xTrainBatch, yTrainOnehotBatch)\n",
    "        lossValid, gradientsValid = model.loss(xValidBatch, yValidOnehotBatch)\n",
    "\n",
    "        # Update the model's weights.\n",
    "        model = sgd(model, gradientsTrain, learningRate)\n",
    "\n",
    "        # Decay the learning rate.\n",
    "        learningRate = calc_decay(batchCount, learningRate, decayRate)\n",
    "\n",
    "        # Save the losses and accuracies.\n",
    "        lossesTrain[batchCount] = lossTrain\n",
    "        accTrain[batchCount] = calc_accuracy(model, xTrainBatch, yTrainBatch)\n",
    "\n",
    "        lossesValid[batchCount] = lossValid\n",
    "        accValid[batchCount] = calc_accuracy(model, xValidBatch, yValidBatch)\n",
    "\n",
    "        # Increment the batch count.\n",
    "        batchCount += 1\n",
    "    \n",
    "    # Shuffle the data so that we get a new set of batches.\n",
    "    random.shuffle(indicesTrain)\n",
    "    xTrain = xTrain[indicesTrain,:]\n",
    "    yTrain = yTrain[indicesTrain]\n",
    "    yTrainOnehot = yTrainOnehot[indicesTrain,:]\n",
    "\n",
    "    random.shuffle(indicesValid)    \n",
    "    xValid = xValid[indicesValid,:]    \n",
    "    yValid = yValid[indicesValid]    \n",
    "    yValidOnehot = yValidOnehot[indicesValid,:]\n",
    "  \n",
    "# Output the results.\n",
    "print(\"Loss: %.4f\\nAccuracy: %.4f\" % (lossesTrain[batchCount - 1], accTrain[batchCount - 1]))\n",
    "fig, ax = pyplot.subplots(1, 2, figsize=(18, 6), dpi=80) # Create a 1 by 2 plot grid.\n",
    "\n",
    "## Plot the loss history.\n",
    "ax[0].set_xlabel('Batch #')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].plot(lossesTrain, label='Training')\n",
    "ax[0].plot(lossesValid, label='Validation')\n",
    "ax[0].legend()\n",
    "\n",
    "## Plot the accuracy history.\n",
    "ax[1].set_xlabel('Batch #')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "ax[1].plot(accTrain, label='Training')\n",
    "ax[1].plot(accValid, label='Validation')\n",
    "ax[1].legend()\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
